{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNPjtNXW8d7U8nxynJc0aFd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ABHI180604/ML-22115/blob/main/lab6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wrL3iJSyNV8F"
      },
      "outputs": [],
      "source": [
        "#A1\n",
        "import numpy as np\n",
        "\n",
        "# Function for the summation unit\n",
        "def summation_unit(inputs, weights):\n",
        "    return np.dot(inputs, weights)\n",
        "\n",
        "# Activation Functions\n",
        "def step_function(x):\n",
        "    return 1 if x >= 0 else 0\n",
        "\n",
        "def bipolar_step_function(x):\n",
        "    return 1 if x >= 0 else -1\n",
        "\n",
        "def sigmoid_function(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def tanh_function(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "def relu_function(x):\n",
        "    return max(0, x)\n",
        "\n",
        "def leaky_relu_function(x, alpha=0.01):\n",
        "    return x if x >= 0 else alpha * x\n",
        "\n",
        "# Error Comparator Unit\n",
        "def comparator_unit(predicted, actual):\n",
        "    return predicted - actual"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#A2\n",
        "# Training data for AND Gate\n",
        "inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "outputs = np.array([0, 0, 0, 1])  # AND Gate outputs\n",
        "weights = np.array([10, 0.2, -0.75])  # Including bias as weight\n",
        "learning_rate = 0.05\n",
        "\n",
        "\n",
        "# Activation function - Step function\n",
        "def perceptron_training(inputs, outputs, weights, learning_rate, max_epochs=1000, error_threshold=0.002):\n",
        "    epochs = 0\n",
        "    errors = []\n",
        "\n",
        "    while epochs < max_epochs:\n",
        "        total_error = 0\n",
        "        for i in range(len(inputs)):\n",
        "            input_with_bias = np.insert(inputs[i], 0, 1)  # Adding bias input (1)\n",
        "            summation = summation_unit(input_with_bias, weights)\n",
        "            prediction = step_function(summation)\n",
        "            error = comparator_unit(prediction, outputs[i])\n",
        "            total_error += error ** 2\n",
        "\n",
        "            # Weight update rule\n",
        "            weights += learning_rate * error * input_with_bias\n",
        "\n",
        "        errors.append(total_error)\n",
        "        epochs += 1\n",
        "\n",
        "        if total_error <= error_threshold:\n",
        "            break\n",
        "\n",
        "    return weights, epochs, errors\n",
        "\n",
        "\n",
        "# Train the perceptron\n",
        "weights, epochs, errors = perceptron_training(inputs, outputs, weights, learning_rate)\n",
        "\n",
        "# Print final weights and epochs taken to converge\n",
        "print(f\"Final weights after training: {weights}\")\n",
        "print(f\"Number of epochs taken to converge: {epochs}\")"
      ],
      "metadata": {
        "id": "XeGw2ANHBJXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#A3\n",
        "# Function to handle different activation functions\n",
        "def perceptron_training_with_activation(inputs, outputs, weights, learning_rate, activation_func, max_epochs=1000,\n",
        "                                        error_threshold=0.002):\n",
        "    epochs = 0\n",
        "    errors = []\n",
        "\n",
        "    while epochs < max_epochs:\n",
        "        total_error = 0\n",
        "        for i in range(len(inputs)):\n",
        "            input_with_bias = np.insert(inputs[i], 0, 1)  # Adding bias input (1)\n",
        "            summation = summation_unit(input_with_bias, weights)\n",
        "            prediction = activation_func(summation)\n",
        "            error = comparator_unit(prediction, outputs[i])\n",
        "            total_error += error ** 2\n",
        "\n",
        "            # Weight update rule\n",
        "            weights += learning_rate * error * input_with_bias\n",
        "\n",
        "        errors.append(total_error)\n",
        "        epochs += 1\n",
        "\n",
        "        if total_error <= error_threshold:\n",
        "            break\n",
        "\n",
        "    return weights, epochs, errors\n",
        "\n",
        "\n",
        "# Comparing different activation functions\n",
        "activation_functions = [step_function, bipolar_step_function, sigmoid_function, relu_function]\n",
        "for func in activation_functions:\n",
        "    weights, epochs, errors = perceptron_training_with_activation(inputs, outputs, weights, learning_rate, func)\n",
        "    print(f\"Activation Function: {func.__name__}, Epochs: {epochs}\")"
      ],
      "metadata": {
        "id": "eLgrZITSBiPf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#A4\n",
        "learning_rates = [0.1 * i for i in range(1, 11)]\n",
        "for lr in learning_rates:\n",
        "    weights = np.array([10, 0.2, -0.75])\n",
        "    weights, epochs, errors = perceptron_training(inputs, outputs, weights, lr)\n",
        "    print(f\"Learning Rate: {lr}, Epochs: {epochs}\")"
      ],
      "metadata": {
        "id": "6xwmFB49BkSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#A5\n",
        "# XOR Gate training data\n",
        "inputs_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "outputs_xor = np.array([0, 1, 1, 0])\n",
        "\n",
        "# Repeat training for XOR gate\n",
        "weights_xor = np.array([10, 0.2, -0.75])\n",
        "weights_xor, epochs_xor, errors_xor = perceptron_training(inputs_xor, outputs_xor, weights_xor, learning_rate)\n",
        "\n",
        "print(f\"Final weights for XOR gate: {weights_xor}\")\n",
        "print(f\"Number of epochs taken to converge for XOR gate: {epochs_xor}\")"
      ],
      "metadata": {
        "id": "FhbQsAH7B6f1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#A6\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# A6: Load the provided dataset\n",
        "data = {\n",
        "    'Customer': ['C_1', 'C_2', 'C_3', 'C_4', 'C_5', 'C_6', 'C_7', 'C_8', 'C_9', 'C_10'],\n",
        "    'Candies': [20, 16, 27, 19, 24, 22, 15, 18, 21, 16],\n",
        "    'Mangoes': [6, 3, 6, 1, 4, 1, 4, 4, 1, 2],\n",
        "    'Milk Packets': [2, 6, 2, 2, 2, 5, 2, 2, 4, 4],\n",
        "    'Payment': [386, 289, 393, 110, 280, 167, 271, 274, 148, 198],\n",
        "    'High Value Tx?': ['Yes', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes', 'Yes', 'No', 'No']\n",
        "}\n",
        "\n",
        "# Create a DataFrame from the dataset\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Convert 'High Value Tx?' to binary labels: 'Yes' -> 1, 'No' -> 0\n",
        "df['High Value Tx?'] = df['High Value Tx?'].apply(lambda x: 1 if x == 'Yes' else 0)\n",
        "\n",
        "# Features and target variable\n",
        "X = df[['Candies', 'Mangoes', 'Milk Packets']]\n",
        "y = df['High Value Tx?']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Initialize the MLPClassifier\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(10,), activation='logistic', solver='adam', learning_rate_init=0.01, max_iter=1000, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "mlp.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict on the test data\n",
        "y_pred = mlp.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Display the model weights\n",
        "print(\"\\nMLPClassifier Weights:\")\n",
        "for i, layer_weights in enumerate(mlp.coefs_):\n",
        "    print(f\"Layer {i + 1} weights:\")"
      ],
      "metadata": {
        "id": "8o1thLqDCHiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#A7\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# Load the provided dataset into a DataFrame\n",
        "data = {\n",
        "    'Customer': ['C_1', 'C_2', 'C_3', 'C_4', 'C_5', 'C_6', 'C_7', 'C_8', 'C_9', 'C_10'],\n",
        "    'Candies': [20, 16, 27, 19, 24, 22, 15, 18, 21, 16],\n",
        "    'Mangoes': [6, 3, 6, 1, 4, 1, 4, 4, 1, 2],\n",
        "    'Milk Packets': [2, 6, 2, 2, 2, 5, 2, 2, 4, 4],\n",
        "    'Payment': [386, 289, 393, 110, 280, 167, 271, 274, 148, 198],\n",
        "    'High Value Tx?': ['Yes', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes', 'Yes', 'No', 'No']\n",
        "}\n",
        "\n",
        "# Create a DataFrame from the dataset\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Convert 'High Value Tx?' to binary labels: 'Yes' -> 1, 'No' -> 0\n",
        "df['High Value Tx?'] = df['High Value Tx?'].apply(lambda x: 1 if x == 'Yes' else 0)\n",
        "\n",
        "# Features and target variable\n",
        "features = df[['Candies', 'Mangoes', 'Milk Packets']].values\n",
        "target = df['High Value Tx?'].values\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "features_scaled = scaler.fit_transform(features)\n",
        "\n",
        "# Function to calculate weights using the pseudo-inverse method\n",
        "def pseudo_inverse_method(X, y):\n",
        "    X_bias = np.hstack([np.ones((X.shape[0], 1)), X])  # Add a bias term\n",
        "    weights_pseudo_inverse = np.linalg.pinv(X_bias).dot(y)\n",
        "    return weights_pseudo_inverse\n",
        "\n",
        "# Calculate weights using pseudo-inverse method\n",
        "weights_pseudo_inverse = pseudo_inverse_method(features_scaled, target)\n",
        "print(f\"Weights obtained using pseudo-inverse method: {weights_pseudo_inverse}\")\n",
        "# Using Perceptron model for comparison\n",
        "perceptron_model = Perceptron(max_iter=1000, random_state=42)\n",
        "perceptron_model.fit(features_scaled, target)\n",
        "perceptron_weights = np.hstack([perceptron_model.intercept_, perceptron_model.coef_[0]])\n",
        "print(f\"Perceptron Weights: {perceptron_weights}\")\n",
        "\n",
        "# Predict and evaluate using perceptron model\n",
        "predictions = perceptron_model.predict(features_scaled)\n",
        "print(\"\\nConfusion Matrix (Perceptron):\")\n",
        "print(confusion_matrix(target, predictions))\n",
        "print(\"\\nClassification Report (Perceptron):\")\n",
        "print(classification_report(target, predictions))"
      ],
      "metadata": {
        "id": "T-VClgaECQeB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#A8\n",
        "# A8: Neural Network with Backpropagation\n",
        "\n",
        "# Initialize weights for the neural network\n",
        "weights_hidden = np.random.rand(3, 2)  # Weights for 2 neurons in the hidden layer (including bias)\n",
        "weights_output = np.random.rand(3)  # Weights for the output layer (including bias)\n",
        "learning_rate = 0.05\n",
        "\n",
        "# Sigmoid Activation function and its derivative\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "# Backpropagation algorithm for a single hidden layer neural network\n",
        "def backpropagation_and_gate(inputs, outputs, weights_hidden, weights_output, learning_rate, max_epochs=1000, error_threshold=0.002):\n",
        "    epochs = 0\n",
        "    errors = []\n",
        "\n",
        "    for epoch in range(max_epochs):\n",
        "        total_error = 0\n",
        "        for i in range(len(inputs)):\n",
        "            # Forward pass\n",
        "            input_with_bias = np.insert(inputs[i], 0, 1)  # Adding bias input (1)\n",
        "            hidden_layer_input = np.dot(input_with_bias, weights_hidden)\n",
        "            hidden_layer_output = sigmoid(hidden_layer_input)\n",
        "\n",
        "            hidden_output_with_bias = np.insert(hidden_layer_output, 0, 1)  # Adding bias for hidden to output layer\n",
        "            final_input = np.dot(hidden_output_with_bias, weights_output)\n",
        "            final_output = sigmoid(final_input)\n",
        "\n",
        "            # Calculate error\n",
        "            error = comparator_unit(final_output, outputs[i])\n",
        "            total_error += error ** 2\n",
        "\n",
        "            # Backward pass (Error Backpropagation)\n",
        "            delta_output = error * sigmoid_derivative(final_output)\n",
        "            delta_hidden = delta_output * weights_output[1:] * sigmoid_derivative(hidden_layer_output)\n",
        "\n",
        "            # Update weights\n",
        "            weights_output += learning_rate * delta_output * hidden_output_with_bias\n",
        "            weights_hidden += learning_rate * np.outer(input_with_bias, delta_hidden)\n",
        "\n",
        "        errors.append(total_error)\n",
        "        epochs += 1\n",
        "\n",
        "        if total_error <= error_threshold:\n",
        "            break\n",
        "\n",
        "    return weights_hidden, weights_output, epochs, errors\n",
        "\n",
        "# Train the neural network using backpropagation for AND gate\n",
        "weights_hidden, weights_output, epochs, errors = backpropagation_and_gate(inputs, outputs, weights_hidden, weights_output, learning_rate)\n",
        "\n",
        "# Print final weights and epochs taken to converge\n",
        "print(f\"Final hidden weights after training: {weights_hidden}\")\n",
        "print(f\"Final output weights after training: {weights_output}\")\n",
        "print(f\"Number of epochs taken to converge: {epochs}\")"
      ],
      "metadata": {
        "id": "HVFYjMEDChme"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#A9\n",
        "import numpy as np\n",
        "\n",
        "# Sigmoid activation function and its derivative\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "# Backpropagation function for XOR gate\n",
        "def backpropagation_xor(inputs, outputs, weights_hidden, weights_output, learning_rate, epochs=10000):\n",
        "    # Initialize error tracking\n",
        "    errors = []\n",
        "    for epoch in range(epochs):\n",
        "        # Forward pass\n",
        "        hidden_input = np.dot(inputs, weights_hidden)\n",
        "        hidden_output = sigmoid(hidden_input)\n",
        "\n",
        "        final_input = np.dot(hidden_output, weights_output)\n",
        "        final_output = sigmoid(final_input)\n",
        "\n",
        "        # Calculate error (difference between expected and predicted outputs)\n",
        "        error = outputs - final_output\n",
        "        errors.append(np.mean(np.abs(error)))\n",
        "\n",
        "        # Backpropagation\n",
        "        output_delta = error * sigmoid_derivative(final_output)\n",
        "        hidden_error = output_delta.dot(weights_output.T)\n",
        "        hidden_delta = hidden_error * sigmoid_derivative(hidden_output)\n",
        "\n",
        "        # Update weights\n",
        "        weights_output += hidden_output.T.dot(output_delta) * learning_rate\n",
        "        weights_hidden += inputs.T.dot(hidden_delta) * learning_rate\n",
        "\n",
        "    return weights_hidden, weights_output, epoch, errors\n",
        "\n",
        "# XOR gate training data\n",
        "inputs_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "outputs_xor = np.array([[0], [1], [1], [0]])\n",
        "\n",
        "# Initialize weights (randomly)\n",
        "np.random.seed(42)  # Set seed for reproducibility\n",
        "weights_hidden = np.random.uniform(size=(2, 2))  # 2 input nodes, 2 hidden nodes\n",
        "weights_output = np.random.uniform(size=(2, 1))  # 2 hidden nodes, 1 output node\n",
        "\n",
        "# Parameters\n",
        "learning_rate = 0.5\n",
        "\n",
        "# Train the neural network using backpropagation for XOR gate\n",
        "weights_hidden_xor, weights_output_xor, epochs_xor, errors_xor = backpropagation_xor(\n",
        "    inputs_xor, outputs_xor, weights_hidden, weights_output, learning_rate\n",
        ")\n",
        "\n",
        "# Print final weights and epochs taken to converge\n",
        "print(f\"Final hidden weights after training for XOR gate:\\n{weights_hidden_xor}\")\n",
        "print(f\"Final output weights after training for XOR gate:\\n{weights_output_xor}\")\n",
        "print(f\"Number of epochs taken to converge for XOR gate: {epochs_xor}\")\n",
        "\n",
        "# Test the network with XOR inputs\n",
        "hidden_output_test = sigmoid(np.dot(inputs_xor, weights_hidden_xor))\n",
        "final_output_test = sigmoid(np.dot(hidden_output_test, weights_output_xor))\n",
        "print(\"\\nPredictions after training:\")"
      ],
      "metadata": {
        "id": "EH0g5sPFC3QA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#A10\n",
        "import numpy as np\n",
        "\n",
        "# Sigmoid activation function and its derivative\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "# Modified backpropagation function for neural networks with 2 output nodes\n",
        "def backpropagation_two_output_nodes(inputs, outputs, weights_hidden, weights_output, learning_rate, max_epochs=1000, error_threshold=0.002):\n",
        "    epochs = 0\n",
        "    errors = []\n",
        "\n",
        "    for epoch in range(max_epochs):\n",
        "        total_error = 0\n",
        "        for i in range(len(inputs)):\n",
        "            # Forward pass\n",
        "            input_with_bias = np.insert(inputs[i], 0, 1)  # Adding bias input (1)\n",
        "            hidden_layer_input = np.dot(input_with_bias, weights_hidden)\n",
        "            hidden_layer_output = sigmoid(hidden_layer_input)\n",
        "\n",
        "            hidden_output_with_bias = np.insert(hidden_layer_output, 0, 1)  # Adding bias for hidden to output layer\n",
        "            final_input = np.dot(hidden_output_with_bias, weights_output)\n",
        "            final_output = sigmoid(final_input)\n",
        "\n",
        "            # Calculate error\n",
        "            error = outputs[i] - final_output\n",
        "            total_error += np.sum(error ** 2)\n",
        "\n",
        "            # Backward pass (Error Backpropagation)\n",
        "            delta_output = error * sigmoid_derivative(final_output)\n",
        "            delta_hidden = np.dot(weights_output[1:], delta_output) * sigmoid_derivative(hidden_layer_output)\n",
        "\n",
        "            # Update weights\n",
        "            weights_output += learning_rate * np.outer(hidden_output_with_bias, delta_output)\n",
        "            weights_hidden += learning_rate * np.outer(input_with_bias, delta_hidden)\n",
        "\n",
        "        errors.append(total_error)\n",
        "        epochs += 1\n",
        "\n",
        "        if total_error <= error_threshold:\n",
        "            break\n",
        "\n",
        "    return weights_hidden, weights_output, epochs, errors\n",
        "\n",
        "# AND gate training data with two output nodes (one-hot encoded)\n",
        "inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  # Inputs for AND gate\n",
        "outputs_and_two_nodes = np.array([[1, 0], [1, 0], [1, 0], [0, 1]])  # Mapping 0 to [1, 0] and 1 to [0, 1]\n",
        "\n",
        "# Initialize weights (randomly)\n",
        "np.random.seed(42)  # Set seed for reproducibility\n",
        "weights_hidden = np.random.rand(3, 3)  # 3 input nodes (including bias), 3 hidden nodes\n",
        "weights_output_two_nodes = np.random.rand(4, 2)  # 3 hidden nodes + 1 bias, 2 output nodes\n",
        "\n",
        "# Parameters\n",
        "learning_rate = 0.5\n",
        "\n",
        "# Train the neural network with 2 output nodes for AND gate\n",
        "weights_hidden_two_nodes, weights_output_two_nodes, epochs_two_nodes, errors_two_nodes = backpropagation_two_output_nodes(\n",
        "    inputs, outputs_and_two_nodes, weights_hidden, weights_output_two_nodes, learning_rate\n",
        ")\n",
        "\n",
        "# Print final weights and epochs taken to converge\n",
        "print(f\"Final hidden weights after training with 2 output nodes:\\n{weights_hidden_two_nodes}\")\n",
        "print(f\"Final output weights after training with 2 output nodes:\\n{weights_output_two_nodes}\")\n",
        "print(f\"Number of epochs taken to converge with 2 output nodes: {epochs_two_nodes}\")\n",
        "\n",
        "# Test the network with AND inputs\n",
        "for input_data in inputs:\n",
        "    input_with_bias = np.insert(input_data, 0, 1)  # Add bias to the input\n",
        "    hidden_layer_output = sigmoid(np.dot(input_with_bias, weights_hidden_two_nodes))\n",
        "    hidden_output_with_bias = np.insert(hidden_layer_output, 0, 1)  # Add bias to hidden layer output\n",
        "    final_output = sigmoid(np.dot(hidden_output_with_bias, weights_output_two_nodes))\n",
        "    print(f\"Input: {input_data} => Output: {final_output}\")"
      ],
      "metadata": {
        "id": "ApSG9nseCoVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#A11\n",
        "\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# A11: Using MLPClassifier for AND Gate\n",
        "mlp_and = MLPClassifier(hidden_layer_sizes=(2,), activation='logistic', learning_rate_init=0.05, max_iter=1000)\n",
        "mlp_and.fit(inputs, outputs)\n",
        "print(f\"MLPClassifier Weights for AND gate: {mlp_and.coefs_}\")\n",
        "\n",
        "# A11: Using MLPClassifier for XOR Gate\n",
        "mlp_xor = MLPClassifier(hidden_layer_sizes=(2,), activation='logistic', learning_rate_init=0.05, max_iter=1000)\n",
        "mlp_xor.fit(inputs_xor, outputs_xor)\n",
        "print(f\"MLPClassifier Weights for XOR gate: {mlp_xor.coefs_}\")\n",
        "MLPClassifier Weights for AND gate: [array([[-4.66439058, -4.70971052],\n",
        "       [-4.70187354, -4.6722469 ]]), array([[-5.52916835],\n",
        "       [-4.94161041]])]\n",
        "MLPClassifier Weights for XOR gate: [array([[-7.21792739, -6.93529282],\n",
        "       [-4.67000142,  4.70247249]]), array([[-5.92964435],\n",
        "       [ 6.08290793]])]\n",
        "C:\\Users\\NISHANTH\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1105: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
        "  y = column_or_1d(y, warn=True)\n",
        "#A12\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Load the dataset\n",
        "file_path = \"D:/DCT_withoutduplicate 3 (1).csv\"\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Assuming the last column is the target and the rest are features\n",
        "X = data.iloc[:, :-1]\n",
        "y = data.iloc[:, -1]\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Standardize the features (important for MLP)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Initialize the MLPClassifier\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(100,), max_iter=300, activation='relu', solver='adam', random_state=42)\n",
        "\n",
        "# Train the model\n",
        "mlp.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = mlp.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "Wm4k-geRDInK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uyUqhTrlDRfx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}